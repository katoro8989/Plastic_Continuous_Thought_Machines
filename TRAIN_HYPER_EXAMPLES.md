# train_hyper.py ä½¿ç”¨ä¾‹

`train_hyper.py`ã¯ã€é€šå¸¸ã®CTMã¨HyperContinuousThoughtMachineã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

## ä¿®æ­£å†…å®¹

### è¿½åŠ ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```bash
--use_hyper              # HyperCTMã‚’ä½¿ç”¨ï¼ˆãƒ•ãƒ©ã‚°ï¼‰
--hyper_layers           # ã©ã®å±¤ã‚’ãƒã‚¤ãƒ‘ãƒ¼åŒ–ã™ã‚‹ã‹
                         # é¸æŠè‚¢: 'none', 'bottleneck', 'down', 'up', 'all'
                         # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'bottleneck'
--hyper_rank             # LoRAãƒ©ãƒ³ã‚¯
                         # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8
```

## ä½¿ç”¨ä¾‹

### 1. é€šå¸¸ã®CTMï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰

```bash
python -m tasks.mazes.train_hyper \
    --model ctm \
    --dataset mazes-small \
    --d_model 1024 \
    --d_input 256 \
    --synapse_depth 8 \
    --heads 4 \
    --n_synch_out 128 \
    --n_synch_action 128 \
    --iterations 50 \
    --memory_length 25 \
    --deep_memory \
    --memory_hidden_dims 32 \
    --backbone_type resnet18-1 \
    --neuron_select_type random-pairing \
    --batch_size 64 \
    --batch_size_test 32 \
    --lr 1e-4 \
    --training_iterations 10000 \
    --track_every 1000 \
    --save_every 2000 \
    --log_dir logs/mazes-small-baseline \
    --device 0
```

### 2. HyperCTM - Bottleneckï¼ˆæ¨å¥¨åˆæœŸè¨­å®šï¼‰

```bash
python -m tasks.mazes.train_hyper \
    --model ctm \
    --use_hyper \
    --hyper_layers bottleneck \
    --hyper_rank 8 \
    --dataset mazes-small \
    --d_model 1024 \
    --d_input 256 \
    --synapse_depth 8 \
    --heads 4 \
    --n_synch_out 128 \
    --n_synch_action 128 \
    --iterations 50 \
    --memory_length 25 \
    --deep_memory \
    --memory_hidden_dims 32 \
    --backbone_type resnet18-1 \
    --neuron_select_type random-pairing \
    --batch_size 64 \
    --batch_size_test 32 \
    --lr 1e-4 \
    --training_iterations 10000 \
    --track_every 1000 \
    --save_every 2000 \
    --log_dir logs/mazes-small-hyper-bottleneck \
    --device 0
```

### 3. HyperCTM - ã™ã¹ã¦ã®å±¤ï¼ˆé«˜æ€§èƒ½ï¼‰

```bash
python -m tasks.mazes.train_hyper \
    --model ctm \
    --use_hyper \
    --hyper_layers all \
    --hyper_rank 16 \
    --dataset mazes-small \
    --d_model 1024 \
    --d_input 256 \
    --synapse_depth 8 \
    --heads 4 \
    --n_synch_out 128 \
    --n_synch_action 128 \
    --iterations 50 \
    --memory_length 25 \
    --deep_memory \
    --memory_hidden_dims 32 \
    --backbone_type resnet18-1 \
    --neuron_select_type random-pairing \
    --batch_size 32 \
    --batch_size_test 32 \
    --lr 1e-4 \
    --training_iterations 10000 \
    --track_every 1000 \
    --save_every 2000 \
    --log_dir logs/mazes-small-hyper-all \
    --device 0
```

### 4. HyperCTM - Downå±¤ã®ã¿

```bash
python -m tasks.mazes.train_hyper \
    --model ctm \
    --use_hyper \
    --hyper_layers down \
    --hyper_rank 8 \
    --dataset mazes-small \
    --d_model 1024 \
    --d_input 256 \
    --synapse_depth 8 \
    --log_dir logs/mazes-small-hyper-down \
    --device 0 \
    ... # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
```

### 5. HyperCTM - Upå±¤ã®ã¿

```bash
python -m tasks.mazes.train_hyper \
    --model ctm \
    --use_hyper \
    --hyper_layers up \
    --hyper_rank 8 \
    --dataset mazes-small \
    --d_model 1024 \
    --d_input 256 \
    --synapse_depth 8 \
    --log_dir logs/mazes-small-hyper-up \
    --device 0 \
    ... # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
```

## æ¯”è¼ƒå®Ÿé¨“ã®è¨­å®š

### å®Ÿé¨“1: Bottleneck vs ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

```bash
# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
python -m tasks.mazes.train_hyper --model ctm \
    --dataset mazes-small --d_model 1024 --d_input 256 \
    --synapse_depth 8 --iterations 50 \
    --log_dir logs/exp1-baseline --device 0

# HyperCTM (Bottleneck)
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers bottleneck --hyper_rank 8 \
    --dataset mazes-small --d_model 1024 --d_input 256 \
    --synapse_depth 8 --iterations 50 \
    --log_dir logs/exp1-hyper-bottleneck --device 0
```

### å®Ÿé¨“2: Rankã®å½±éŸ¿

```bash
# Rank 4
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers bottleneck --hyper_rank 4 \
    --log_dir logs/exp2-rank4 --device 0 ...

# Rank 8
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers bottleneck --hyper_rank 8 \
    --log_dir logs/exp2-rank8 --device 0 ...

# Rank 16
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers bottleneck --hyper_rank 16 \
    --log_dir logs/exp2-rank16 --device 0 ...
```

### å®Ÿé¨“3: å±¤ã®é¸æŠ

```bash
# Bottleneck
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers bottleneck --log_dir logs/exp3-bottleneck --device 0 ...

# Down
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers down --log_dir logs/exp3-down --device 0 ...

# Up
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers up --log_dir logs/exp3-up --device 0 ...

# All
python -m tasks.mazes.train_hyper --model ctm --use_hyper \
    --hyper_layers all --log_dir logs/exp3-all --device 0 ...
```

## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª

è¨“ç·´é–‹å§‹æ™‚ã«ä»¥ä¸‹ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼š

```
Using HyperContinuousThoughtMachine with hyper_layers=bottleneck, hyper_rank=8
Using neuron select type: random-pairing
Synch representation size action: 128
Synch representation size out: 128
Total params: 12,345,678
```

## ãƒ­ã‚°ã®ç¢ºèª

è¨“ç·´ä¸­ã«ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼š

```
logs/your-experiment/
â”œâ”€â”€ accuracies.png              # ç²¾åº¦ã®ãƒ—ãƒ­ãƒƒãƒˆ
â”œâ”€â”€ losses.png                  # æå¤±ã®ãƒ—ãƒ­ãƒƒãƒˆ
â”œâ”€â”€ prediction.gif              # äºˆæ¸¬ã®å¯è¦–åŒ–
â”œâ”€â”€ neural_dynamics_other.pdf   # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å‹•æ…‹
â”œâ”€â”€ checkpoint.pt               # ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ args.txt                    # ä½¿ç”¨ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
â””â”€â”€ repo_state.zip              # ã‚³ãƒ¼ãƒ‰ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ¡ãƒ¢ãƒªä¸è¶³

```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
--batch_size 32 \
--batch_size_test 16

# ã¾ãŸã¯ hyper_layers ã‚’ bottleneck ã«
--hyper_layers bottleneck
```

### å­¦ç¿’ãŒä¸å®‰å®š

```bash
# Rankã‚’ä¸‹ã’ã‚‹
--hyper_rank 4

# ã¾ãŸã¯å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹
--lr 5e-5
```

### Compile ã‚¨ãƒ©ãƒ¼

```bash
# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’ç„¡åŠ¹åŒ–
# --do_compile ãƒ•ãƒ©ã‚°ã‚’å‰Šé™¤
```

## æ¨å¥¨ã•ã‚Œã‚‹å®Ÿé¨“é †åº

1. **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹**
   ```bash
   python -m tasks.mazes.train_hyper --model ctm \
       --dataset mazes-small --training_iterations 10000 \
       --log_dir logs/baseline
   ```

2. **Bottleneck HyperCTM**
   ```bash
   python -m tasks.mazes.train_hyper --model ctm --use_hyper \
       --hyper_layers bottleneck --hyper_rank 8 \
       --dataset mazes-small --training_iterations 10000 \
       --log_dir logs/hyper-bottleneck
   ```

3. **æ€§èƒ½ãŒè‰¯ã‘ã‚Œã°ã€ã‚ˆã‚Šå¤šãã®å±¤ã‚’è©¦ã™**
   ```bash
   python -m tasks.mazes.train_hyper --model ctm --use_hyper \
       --hyper_layers all --hyper_rank 8 \
       --dataset mazes-small --training_iterations 10000 \
       --log_dir logs/hyper-all
   ```

4. **Rankã®èª¿æ•´**
   - æ€§èƒ½ãŒæ‚ªã„ â†’ rank ã‚’ä¸Šã’ã‚‹ (8 â†’ 16)
   - ãƒ¡ãƒ¢ãƒªä¸è¶³ â†’ rank ã‚’ä¸‹ã’ã‚‹ (8 â†’ 4)

## æ³¨æ„äº‹é …

1. **synapse_depth**: 
   - `synapse_depth=1`ã®å ´åˆã€ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“
   - æ¨å¥¨: `synapse_depth >= 4`

2. **--use_hyper ãƒ•ãƒ©ã‚°**:
   - ã“ã®ãƒ•ãƒ©ã‚°ãŒãªã„ã¨é€šå¸¸ã®CTMãŒä½¿ç”¨ã•ã‚Œã¾ã™
   - `--hyper_layers`ã¨`--hyper_rank`ã¯`--use_hyper`ãŒã‚ã‚‹å ´åˆã®ã¿æœ‰åŠ¹

3. **ãƒ‡ãƒã‚¤ã‚¹**:
   - `--device 0`: GPU 0ã‚’ä½¿ç”¨
   - `--device -1`: CPUã‚’ä½¿ç”¨ï¼ˆéæ¨å¥¨ã€éå¸¸ã«é…ã„ï¼‰

4. **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å†é–‹**:
   ```bash
   python -m tasks.mazes.train_hyper ... \
       --reload \
       --log_dir logs/existing-experiment
   ```

## ã¾ã¨ã‚

- âœ… **åˆå¿ƒè€…**: `--use_hyper --hyper_layers bottleneck --hyper_rank 8`
- âœ… **æ¨™æº–**: `--use_hyper --hyper_layers bottleneck --hyper_rank 8`
- ğŸš€ **é«˜æ€§èƒ½**: `--use_hyper --hyper_layers all --hyper_rank 16`
- ğŸ’¡ **è»½é‡**: `--use_hyper --hyper_layers bottleneck --hyper_rank 4`
- ğŸ“Š **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: `--use_hyper`ãªã—ï¼ˆé€šå¸¸ã®CTMï¼‰

