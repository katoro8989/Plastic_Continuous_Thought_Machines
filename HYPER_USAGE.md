# HyperContinuousThoughtMachine ä½¿ç”¨ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

`HyperContinuousThoughtMachine`ã¯ã€é€šå¸¸ã®CTMã«ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
å…¥åŠ›çŠ¶æ…‹ã«å¿œã˜ã¦Synapseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é‡ã¿ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠæŸ”è»Ÿãªæ€è€ƒå‡¦ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## å®Ÿè£…å®Œäº†å†…å®¹

### âœ… å®Ÿè£…æ¸ˆã¿

1. **HyperLoRALinear** (`models/modules.py`)
   - Low-rankå‹•çš„é‡ã¿èª¿æ•´
   - å­¦ç¿’å¯èƒ½ãªGateæ©Ÿæ§‹
   - è¨ˆç®—æœ€é©åŒ–æ¸ˆã¿

2. **HyperSynapseUNET** (`models/modules.py`)
   - æŒ‡å®šå±¤ã®ã¿ãƒã‚¤ãƒ‘ãƒ¼åŒ–å¯èƒ½
   - é€šå¸¸ã®SynapseUNETã¨åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

3. **HyperContinuousThoughtMachine** (`models/ctm.py`)
   - CTMã‚’ç¶™æ‰¿ã—ãŸå®Ÿè£…
   - 2ã¤ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿

## åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
from models.ctm import HyperContinuousThoughtMachine

model = HyperContinuousThoughtMachine(
    # é€šå¸¸ã®CTMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    iterations=50,
    d_model=1024,
    d_input=256,
    heads=4,
    n_synch_out=128,
    n_synch_action=128,
    synapse_depth=8,
    memory_length=25,
    deep_nlms=True,
    memory_hidden_dims=32,
    do_layernorm_nlm=False,
    backbone_type='resnet18-1',
    positional_embedding_type='none',
    out_dims=250,
    neuron_select_type='random-pairing',
    dropout=0.0,
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    hyper_layers='bottleneck',  # ã©ã®å±¤ã‚’ãƒã‚¤ãƒ‘ãƒ¼åŒ–ã™ã‚‹ã‹
    hyper_rank=8,               # LoRAãƒ©ãƒ³ã‚¯
)

# é€šå¸¸ã®CTMã¨å…¨ãåŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
predictions, certainties, _ = model(x)
```

## ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### `hyper_layers` (é‡è¦ï¼)

ã©ã®å±¤ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’é©ç”¨ã™ã‚‹ã‹ã‚’åˆ¶å¾¡ï¼š

- **`'bottleneck'`** (æ¨å¥¨): ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å±¤ã®ã¿
  - æœ€ã‚‚æƒ…å ±ãŒåœ§ç¸®ã•ã‚Œã‚‹å±¤
  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ : æœ€å°
  - åŠ¹æœ: ä¸­ç¨‹åº¦
  
- **`'down'`**: Down projectionå±¤ã™ã¹ã¦
  - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰éç¨‹ã‚’å‹•çš„åŒ–
  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ : ä¸­
  
- **`'up'`**: Up projectionå±¤ã™ã¹ã¦
  - ãƒ‡ã‚³ãƒ¼ãƒ‰éç¨‹ã‚’å‹•çš„åŒ–
  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ : ä¸­
  
- **`'all'`**: ã™ã¹ã¦ã®å±¤
  - æœ€å¤§ã®æŸ”è»Ÿæ€§
  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ : å¤§
  - è¨ˆç®—ã‚³ã‚¹ãƒˆ: é«˜
  
- **`'none'`**: ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãªã—
  - é€šå¸¸ã®SynapseUNETã¨åŒã˜

### `hyper_rank`

Low-rankåˆ†è§£ã®ãƒ©ãƒ³ã‚¯ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8)

- **ä½ã„ (2-4)**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ã€è¡¨ç¾åŠ›ä½ã‚
- **ä¸­ç¨‹åº¦ (8-16)**: ãƒãƒ©ãƒ³ã‚¹è‰¯ã„
- **é«˜ã„ (32+)**: è¡¨ç¾åŠ›é«˜ã„ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢—åŠ 

## è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®çµ±åˆä¾‹

### Option 1: ç›´æ¥ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£

`tasks/mazes/train.py` ã®å¤‰æ›´ä¾‹ï¼š

```python
# Importè¿½åŠ 
from models.ctm import ContinuousThoughtMachine, HyperContinuousThoughtMachine

# argparseè¿½åŠ  (line 73ä»˜è¿‘)
# CTM specific
parser.add_argument('--synapse_depth', type=int, default=8, ...)
# ä»¥ä¸‹ã‚’è¿½åŠ ï¼š
parser.add_argument('--use_hyper', action='store_true', 
                    help='Use HyperContinuousThoughtMachine instead of CTM')
parser.add_argument('--hyper_layers', type=str, default='bottleneck',
                    choices=['none', 'bottleneck', 'down', 'up', 'all'],
                    help='Which layers to apply hypernetwork to')
parser.add_argument('--hyper_rank', type=int, default=8,
                    help='Rank for LoRA decomposition in hypernetwork')

# ãƒ¢ãƒ‡ãƒ«ä½œæˆéƒ¨åˆ† (line 166ä»˜è¿‘)
if args.model == 'ctm':
    # å…ƒã®ã‚³ãƒ¼ãƒ‰
    model_class = HyperContinuousThoughtMachine if args.use_hyper else ContinuousThoughtMachine
    
    model_kwargs = {
        'iterations': args.iterations,
        'd_model': args.d_model,
        'd_input': args.d_input,
        'heads': args.heads,
        'n_synch_out': args.n_synch_out,
        'n_synch_action': args.n_synch_action,
        'synapse_depth': args.synapse_depth,
        'memory_length': args.memory_length,
        'deep_nlms': args.deep_memory,
        'memory_hidden_dims': args.memory_hidden_dims,
        'do_layernorm_nlm': args.do_normalisation,
        'backbone_type': args.backbone_type,
        'positional_embedding_type': args.positional_embedding_type,
        'out_dims': args.out_dims,
        'prediction_reshaper': prediction_reshaper,
        'dropout': args.dropout,
        'dropout_nlm': args.dropout_nlm,
        'neuron_select_type': args.neuron_select_type,
        'n_random_pairing_self': args.n_random_pairing_self,
    }
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½¿ç”¨æ™‚ã®ã¿è¿½åŠ 
    if args.use_hyper:
        model_kwargs['hyper_layers'] = args.hyper_layers
        model_kwargs['hyper_rank'] = args.hyper_rank
    
    model = model_class(**model_kwargs).to(device)
```

### Option 2: Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç›´æ¥ä½¿ç”¨

```python
# test_hyper.py
import torch
from models.ctm import HyperContinuousThoughtMachine

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = HyperContinuousThoughtMachine(
    iterations=50,
    d_model=1024,
    d_input=256,
    heads=4,
    n_synch_out=128,
    n_synch_action=128,
    synapse_depth=8,
    memory_length=25,
    deep_nlms=True,
    memory_hidden_dims=32,
    do_layernorm_nlm=False,
    backbone_type='resnet18-1',
    positional_embedding_type='none',
    out_dims=250,
    neuron_select_type='random-pairing',
    hyper_layers='bottleneck',
    hyper_rank=8,
).cuda()

# ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
x = torch.randn(4, 3, 99, 99).cuda()
predictions, certainties, _ = model(x)

print(f"Predictions shape: {predictions.shape}")  # (4, 250, 50)
print(f"Certainties shape: {certainties.shape}")  # (4, 2, 50)
```

## ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ä¾‹

```bash
# é€šå¸¸ã®CTM
python -m tasks.mazes.train \
    --model ctm \
    --d_model 1024 \
    --d_input 256 \
    --synapse_depth 8 \
    --hyper_layers none \
    ...

# Bottleneckã®ã¿ãƒã‚¤ãƒ‘ãƒ¼åŒ–ï¼ˆæ¨å¥¨ï¼‰
python -m tasks.mazes.train \
    --model ctm \
    --use_hyper \
    --hyper_layers bottleneck \
    --hyper_rank 8 \
    --d_model 1024 \
    --d_input 256 \
    --synapse_depth 8 \
    ...

# ã™ã¹ã¦ã®å±¤ã‚’ãƒã‚¤ãƒ‘ãƒ¼åŒ–
python -m tasks.mazes.train \
    --model ctm \
    --use_hyper \
    --hyper_layers all \
    --hyper_rank 8 \
    ...
```

## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¯”è¼ƒ

ä¾‹: `d_model=1024`, `synapse_depth=8`, `rank=8`

| è¨­å®š | è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° (æ¦‚ç®—) |
|------|-------------------------|
| `hyper_layers='none'` | 0 (é€šå¸¸ã®CTM) |
| `hyper_layers='bottleneck'` | ~100K |
| `hyper_layers='down'` | ~500K |
| `hyper_layers='up'` | ~500K |
| `hyper_layers='all'` | ~1M |

## æ¨å¥¨è¨­å®š

### åˆæœŸå®Ÿé¨“
```python
hyper_layers='bottleneck'
hyper_rank=8
```
- ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ãã€åŠ¹æœã‚’ç¢ºèªã—ã‚„ã™ã„

### é«˜æ€§èƒ½
```python
hyper_layers='all'
hyper_rank=16
```
- æœ€å¤§ã®æŸ”è»Ÿæ€§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜

### è»½é‡
```python
hyper_layers='bottleneck'
hyper_rank=4
```
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€å°ã€åŠ¹æœã¯é™å®šçš„

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Q: å­¦ç¿’ãŒä¸å®‰å®šã«ãªã£ãŸ
A: `hyper_rank`ã‚’ä¸‹ã’ã‚‹ã‹ã€`hyper_layers='bottleneck'`ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„

### Q: ãƒ¡ãƒ¢ãƒªä¸è¶³
A: `hyper_layers='bottleneck'`ã‚’ä½¿ã†ã‹ã€`batch_size`ã‚’æ¸›ã‚‰ã—ã¦ãã ã•ã„

### Q: é€šå¸¸ã®CTMã¨ã®æ€§èƒ½å·®ãŒãªã„
A: ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„:
- `hyper_rank`ã‚’å¢—ã‚„ã™ (8 â†’ 16)
- `hyper_layers='all'`ã«å¤‰æ›´
- ã‚¿ã‚¹ã‚¯ãŒãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ©æµã‚’å—ã‘ã«ãã„å¯èƒ½æ€§

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… **å®Ÿè£…å®Œäº†**: HyperLoRALinear, HyperSynapseUNET, HyperContinuousThoughtMachine
2. ğŸ”„ **çµ±åˆä½œæ¥­**: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®çµ±åˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
3. ğŸ§ª **å®Ÿé¨“**: Bottleneckã§ã®æ€§èƒ½æ¤œè¨¼
4. ğŸ“Š **åˆ†æ**: å‹•çš„é‡ã¿ã®å¯è¦–åŒ–
5. ğŸš€ **æ‹¡å¼µ**: NLM (SuperLinear) ã¸ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆé©ç”¨

## æŠ€è¡“è©³ç´°

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Input (x) â†’ [Context Network] â†’ Gate, U, V
                                   â†“
Base Layer (W_base) â† x       LoRA (U @ V^T)
         â†“                          â†“
    out_base              Gate * (U @ (V^T @ x))
         â†“                          â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (+) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
                   Output
```

### è¨ˆç®—é‡

- Base Layer: O(B * In * Out)
- LoRA Path: O(B * In * Rank + B * Out * Rank)
- Total: O(B * (In * Out + In * Rank + Out * Rank))

RankãŒå°ã•ã„ãŸã‚ã€é€šå¸¸ã®ç·šå½¢å±¤ã¨ã»ã¼åŒã˜è¨ˆç®—é‡ã§ã™ã€‚

